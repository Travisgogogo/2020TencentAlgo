{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3\"\n",
    "config_tf = tf.ConfigProto()\n",
    "config_tf.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config_tf)\n",
    "\n",
    "import keras\n",
    "from keras.backend import reverse\n",
    "from keras.initializers import glorot_normal,glorot_uniform, lecun_uniform,Orthogonal\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, SpatialDropout1D, Multiply, concatenate, Activation, Average\n",
    "from keras.layers import Dropout, BatchNormalization, Concatenate, Conv1D, Flatten, Masking\n",
    "from keras.layers import Bidirectional, CuDNNLSTM, CuDNNGRU, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from keras.engine.base_layer import Layer\n",
    "from transformer import *\n",
    "from keras.utils.training_utils import multi_gpu_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    embeddingSize = 256\n",
    "    sequenceLength = 90\n",
    "    input_channels = 5\n",
    "    istest = True\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 396 ms, sys: 4.22 s, total: 4.62 s\n",
      "Wall time: 4.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "label_1 = pd.read_csv('./data/train_preliminary/user.csv')\n",
    "label_2 = pd.read_csv('./data/train_semi_final/user.csv')\n",
    "label = pd.concat([label_1, label_2], axis=0).reset_index(drop=True)\n",
    "mats_train = []\n",
    "mats_test = []\n",
    "for col in tqdm(['creative_id', 'ad_id', 'advertiser_id', 'product_id', 'industry']):    \n",
    "    mats_train.append(np.load('./inputs/{}_inputs_train.npy'.format(col)))\n",
    "    mats_test.append(np.load('./inputs/{}_inputs_test.npy'.format(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(Sequence):\n",
    "    def __init__(self, xs, y, batch_size=128, shuffle=True):\n",
    "        self.xs = xs\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.size = xs[0].shape[0]\n",
    "        self.shuffle = shuffle\n",
    "        if self.shuffle:\n",
    "            state = np.random.get_state()\n",
    "            for x in self.xs:\n",
    "                np.random.set_state(state)\n",
    "                np.random.shuffle(x)\n",
    "            np.random.set_state(state)\n",
    "            np.random.shuffle(self.y)\n",
    "         \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.size / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx = np.arange(idx * self.batch_size, min((idx + 1) * self.batch_size, self.size))\n",
    "        batch_xs = [x[batch_idx] for x in self.xs]\n",
    "        batch_y = self.y[batch_idx]\n",
    "        # shuffle\n",
    "        if self.shuffle:\n",
    "            x = []\n",
    "            for i in range(len(batch_xs)):\n",
    "                x.append(batch_xs[i].copy())\n",
    "            for i in range(len(x[0])):\n",
    "                p = np.random.rand()\n",
    "                if p < 0.8:\n",
    "                    state = np.random.get_state()\n",
    "                    for j in range(len(batch_xs)):\n",
    "                        np.random.set_state(state)\n",
    "                        np.random.shuffle(x[j][i])\n",
    "            batch_xs = x        \n",
    "        return batch_xs, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Checkpoint(ModelCheckpoint):\n",
    "    \n",
    "    def __init__(self, model_saved, filepath, monitor='val_loss', verbose=0,\n",
    "                 save_best_only=False, save_weights_only=False,\n",
    "                 mode='auto', period=1):\n",
    "        ModelCheckpoint.__init__(self, filepath, monitor='val_loss', verbose=0,\n",
    "                 save_best_only=False, save_weights_only=False,\n",
    "                 mode='auto', period=1)\n",
    "        self.model = model_saved\n",
    "        \n",
    "    def set_model(self, model):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception():\n",
    "    def __init__(self, use_relu=True, use_norm=True):\n",
    "        self.use_relu = use_relu\n",
    "        self.use_norm = use_norm\n",
    "\n",
    "    def __call__(self, input_x):\n",
    "        self.branch1 = Sequential()\n",
    "        self.branch1.add(Conv1D(filters=128, kernel_size=1, strides=1))\n",
    "        \n",
    "        self.branch2 = Sequential()\n",
    "        self.branch2.add(Conv1D(filters=128, kernel_size=2, strides=1))\n",
    "        self.branch2.add(BatchNormalization())\n",
    "        self.branch2.add(ReLU())\n",
    "        self.branch2.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same'))\n",
    "        \n",
    "        self.branch3 = Sequential()\n",
    "        self.branch3.add(Conv1D(filters=128, kernel_size=3, strides=1))\n",
    "        self.branch3.add(BatchNormalization())\n",
    "        self.branch3.add(ReLU())\n",
    "        self.branch3.add(Conv1D(filters=128, kernel_size=5, strides=1, padding='same'))\n",
    "        \n",
    "        self.branch4 = Sequential()\n",
    "        self.branch4.add(Conv1D(filters=128, kernel_size=3, strides=1, padding='same'))\n",
    "        \n",
    "#         self.branch5 = Sequential()\n",
    "#         self.branch5.add(Conv1D(filters=128, kernel_size=2, strides=1))\n",
    "        \n",
    "#         self.branch6 = Sequential()\n",
    "#         self.branch6.add(Conv1D(filters=128, kernel_size=2, strides=1))\n",
    "#         self.branch6.add(BatchNormalization())\n",
    "#         self.branch6.add(ReLU())\n",
    "#         self.branch6.add(Conv1D(filters=128, kernel_size=7, strides=1, padding='same'))\n",
    "        \n",
    "        branch1 = self.branch1(input_x)\n",
    "        branch2 = self.branch2(input_x)\n",
    "        branch3 = self.branch3(input_x)\n",
    "        branch4 = self.branch4(input_x)\n",
    "#         branch5 = self.branch5(input_x)\n",
    "#         branch6 = self.branch6(input_x)\n",
    "        \n",
    "        ret = Concatenate(axis=1)([branch1, branch2, branch3, branch4])\n",
    "        ret = BatchNormalization()(ret)\n",
    "        ret = ReLU()(ret)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(config, n_cls=10):\n",
    "    cols = ['creative_id', 'ad_id', 'advertiser_id', 'product_id', 'industry']\n",
    "    n_in = len(cols)\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    max_len = []\n",
    "    for i in range(n_in):\n",
    "        We = np.load('./w2v_256_10/{}_embedding_weight.npy'.format(cols[i]))\n",
    "        We = np.vstack([We, np.zeros(config.embeddingSize)])\n",
    "        inp = Input(shape=(config.sequenceLength,), dtype=\"int32\")\n",
    "        x = Embedding(We.shape[0], We.shape[1], weights=[We], trainable=False)(inp)\n",
    "        inputs.append(inp)\n",
    "        outputs.append(x)\n",
    "        del We\n",
    "        gc.collect()\n",
    "\n",
    "    embedding_model = Model(inputs, outputs)        \n",
    "    \n",
    "    inputs = []\n",
    "    for i in range(n_in):\n",
    "        inp = Input(shape=(config.sequenceLength, config.embeddingSize, ))\n",
    "        inputs.append(inp)\n",
    "        \n",
    "    all_input = Concatenate()(inputs)\n",
    "    all_input = SpatialDropout1D(0.2)(all_input)\n",
    "    lstm1 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(all_input)\n",
    "    lstm2 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(lstm1)\n",
    "    inc1 = Inception()(lstm1)\n",
    "    inc2 = Inception()(lstm2)\n",
    "    pool_1 = GlobalMaxPooling1D()(lstm1)\n",
    "    pool_2 = GlobalMaxPooling1D()(lstm2)\n",
    "    pool_3 = GlobalMaxPooling1D()(inc1)\n",
    "    pool_4 = GlobalMaxPooling1D()(inc2)\n",
    "    pool = Concatenate()([pool_1,pool_3, pool_2, pool_4])\n",
    "    pool = Dropout(0.2)(pool)\n",
    "    \n",
    "    outputs = Dense(n_cls, activation='softmax')(pool)\n",
    "    lstm_model = Model(inputs, outputs)\n",
    "    model = Model(embedding_model.inputs, lstm_model(embedding_model.outputs))\n",
    "\n",
    "    return model, lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(config, fold, n_cls, xs_trn, y_trn, xs_val, y_val, xs_test=None, batch_size=256, logger=None):\n",
    "    tf.reset_default_graph()\n",
    "    K.clear_session()\n",
    "    trn_generator = DataSequence(xs_trn, y_trn, batch_size=batch_size, shuffle=True)\n",
    "    val_generator = DataSequence(xs_val, y_val, batch_size=batch_size, shuffle=False)\n",
    "    if config.istest:\n",
    "        test_generator = DataSequence(xs_test, np.zeros(xs_test[0].shape[0]), batch_size=batch_size, shuffle=False)\n",
    "    model, lstm_model = LSTM(config, n_cls=n_cls)\n",
    "    model = multi_gpu_model(model, gpus=4)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    ckp_cb = Checkpoint(lstm_model, './model/fold_{}_lstm_age_weight1.pkl'.format(fold), save_best_only=True, save_weights_only=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, min_lr=0, verbose=1)\n",
    "    csvlogger = CSVLogger('./foldfoldfold_{}_logger.csv'.format(fold))\n",
    "    es_cb = EarlyStopping(monitor='val_acc', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "    history = model.fit_generator(generator=trn_generator, \n",
    "                                epochs=15, \n",
    "                                verbose=1, \n",
    "                                workers=8,\n",
    "                                use_multiprocessing=True,\n",
    "                                max_q_size=48,\n",
    "                                validation_data=val_generator,\n",
    "                                callbacks=[ckp_cb, reduce_lr,csvlogger])\n",
    "    lstm_model.load_weights('./model/fold_{}_lstm_age_weight.pkl'.format(fold))\n",
    "    y_pred = model.predict_generator(val_generator)\n",
    "    if config.istest:\n",
    "        print(\"test set predicting\")\n",
    "        y_test = model.predict_generator(test_generator)\n",
    "    del model\n",
    "    if config.istest:\n",
    "        return y_pred, y_test\n",
    "    else:\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n",
      "2820000 180000\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:92: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:95: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:99: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:514: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4076: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3363: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/15\n",
      "1376/1377 [============================>.] - ETA: 0s - loss: 1.3976 - acc: 0.4267\n",
      "1377/1377 [==============================] - 269s 196ms/step - loss: 1.3975 - acc: 0.4267 - val_loss: 1.2745 - val_acc: 0.4734\n",
      "Epoch 2/15\n",
      "1377/1377 [==============================] - 242s 175ms/step - loss: 1.2829 - acc: 0.4693 - val_loss: 1.2396 - val_acc: 0.4857\n",
      "Epoch 3/15\n",
      "1377/1377 [==============================] - 239s 173ms/step - loss: 1.2527 - acc: 0.4805 - val_loss: 1.2201 - val_acc: 0.4935\n",
      "Epoch 4/15\n",
      "1377/1377 [==============================] - 239s 173ms/step - loss: 1.2351 - acc: 0.4874 - val_loss: 1.2077 - val_acc: 0.4988\n",
      "Epoch 5/15\n",
      "1377/1377 [==============================] - 239s 174ms/step - loss: 1.2224 - acc: 0.4921 - val_loss: 1.2021 - val_acc: 0.5002\n",
      "Epoch 6/15\n",
      "1377/1377 [==============================] - 242s 176ms/step - loss: 1.2127 - acc: 0.4960 - val_loss: 1.1993 - val_acc: 0.5021\n",
      "Epoch 7/15\n",
      "1377/1377 [==============================] - 242s 176ms/step - loss: 1.2049 - acc: 0.4989 - val_loss: 1.1884 - val_acc: 0.5060\n",
      "Epoch 8/15\n",
      "1377/1377 [==============================] - 242s 176ms/step - loss: 1.1982 - acc: 0.5015 - val_loss: 1.1854 - val_acc: 0.5073\n",
      "Epoch 9/15\n",
      "1377/1377 [==============================] - 242s 176ms/step - loss: 1.1925 - acc: 0.5037 - val_loss: 1.1823 - val_acc: 0.5083\n",
      "Epoch 10/15\n",
      "1377/1377 [==============================] - 242s 176ms/step - loss: 1.1875 - acc: 0.5055 - val_loss: 1.1822 - val_acc: 0.5083\n",
      "Epoch 11/15\n",
      "1377/1377 [==============================] - 239s 174ms/step - loss: 1.1832 - acc: 0.5071 - val_loss: 1.1765 - val_acc: 0.5115\n",
      "Epoch 12/15\n",
      "1377/1377 [==============================] - 242s 176ms/step - loss: 1.1794 - acc: 0.5087 - val_loss: 1.1793 - val_acc: 0.5094\n",
      "Epoch 13/15\n",
      "1377/1377 [==============================] - 242s 176ms/step - loss: 1.1757 - acc: 0.5099 - val_loss: 1.1759 - val_acc: 0.5107\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 14/15\n",
      "1377/1377 [==============================] - 239s 173ms/step - loss: 1.1651 - acc: 0.5145 - val_loss: 1.1694 - val_acc: 0.5134\n",
      "Epoch 15/15\n",
      "1377/1377 [==============================] - 242s 176ms/step - loss: 1.1616 - acc: 0.5156 - val_loss: 1.1719 - val_acc: 0.5127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [1:02:09, 1864.54s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 3 layers into a model with 13 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-38738b296a97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#     gc.collect()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mistest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmats_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmats_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-a7b69c1cbc7f>\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m(config, fold, n_cls, xs_trn, y_trn, xs_val, y_val, xs_test, batch_size, logger)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                 callbacks=[ckp_cb, reduce_lr,csvlogger])\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/fold_{}_lstm_age_weight.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mistest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1180\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m    899\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 3 layers into a model with 13 layers."
     ]
    }
   ],
   "source": [
    "tmp_data = pd.read_csv('./data/train_preliminary/user.csv')\n",
    "\n",
    "batch_size = 2048\n",
    "y = label['age'].values\n",
    "yc = to_categorical(y-1)\n",
    "num_classes = 10\n",
    "best_score = 0\n",
    "oof = []\n",
    "cv_pred_stack = np.zeros((mats_train[0].shape[0],num_classes))\n",
    "if config.istest:\n",
    "    test_pred_stack = np.zeros((mats_test[0].shape[0],num_classes))\n",
    "kfold = StratifiedKFold(n_splits=5,random_state=2020)\n",
    "for index, (idx_trn, idx_val) in tqdm(enumerate(kfold.split(tmp_data, tmp_data['age']))):  \n",
    "    if index == 2:\n",
    "        print(\"Fold %d\"  % index)\n",
    "        all_idx = np.arange(len(mats_train[0]))\n",
    "        subtraction = list((set(all_idx).difference(set(idx_val))))\n",
    "        xs_trn = [x[subtraction] for x in mats_train]\n",
    "        xs_val = [x[idx_val] for x in mats_train]\n",
    "        print(len(xs_trn[0]), len(xs_val[0]))\n",
    "        y_trn, y_val = yc[subtraction], yc[idx_val]\n",
    "    #     del mats_train\n",
    "    #     gc.collect()\n",
    "        if config.istest:\n",
    "            y_pred, y_test = train_eval(config, index, num_classes, xs_trn, y_trn, xs_val, y_val, mats_test, batch_size) \n",
    "        else:\n",
    "            y_pred = train_eval(config, index, num_classes, xs_trn, y_trn, xs_val, y_val, mats_test, batch_size) \n",
    "\n",
    "        cv_pred_stack[idx_val] = y_pred\n",
    "        if config.istest:\n",
    "            print(\"result saved\")\n",
    "            test_pred_stack += y_test / 5\n",
    "\n",
    "acc = accuracy_score(y, np.argmax(cv_pred_stack,axis=1)+1)\n",
    "print('score: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./fold0_test_pred.npy', test_pred_stack)\n",
    "np.save('./fold0_val_pred.npy', cv_pred_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
