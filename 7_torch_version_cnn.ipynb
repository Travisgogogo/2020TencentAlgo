{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch as t\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:09<00:00, 37.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 496 ms, sys: 7.93 s, total: 8.43 s\n",
      "Wall time: 3min 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "label_1 = pd.read_csv('./data/train_preliminary/user.csv')\n",
    "label_2 = pd.read_csv('./data/train_semi_final/user.csv')\n",
    "label = pd.concat([label_1, label_2], axis=0).reset_index(drop=True)\n",
    "mats_train = []\n",
    "mats_test = []\n",
    "for col in tqdm(['creative_id', 'ad_id', 'advertiser_id', 'product_id', 'industry']):    \n",
    "    mats_train.append(np.load('./inputs_new/{}_inputs_train.npy'.format(col)))\n",
    "    mats_test.append(np.load('./inputs_new/{}_inputs_test.npy'.format(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def get_logger(filename, verbosity=1, name=None):\n",
    "    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level_dict[verbosity])\n",
    "\n",
    "    fh = logging.FileHandler(filename, \"w\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    sh = logging.StreamHandler()\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(sh)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self,cin,co,relu=True,norm=True):\n",
    "        super(Inception, self).__init__()\n",
    "        assert(co%4==0)\n",
    "        cos=[co//4]*4\n",
    "        self.activa=nn.Sequential()\n",
    "        if norm:self.activa.add_module('norm',nn.BatchNorm1d(co))\n",
    "        if relu:self.activa.add_module('relu',nn.ReLU(True))\n",
    "        self.branch1 =nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(cin,cos[0], 1,stride=1)),\n",
    "            ])) \n",
    "        self.branch2 =nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(cin,cos[1], 1)),\n",
    "            ('norm1', nn.BatchNorm1d(cos[1])),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('conv3', nn.Conv1d(cos[1],cos[1], 3,stride=1,padding=1)),\n",
    "            ]))\n",
    "        self.branch3 =nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(cin,cos[2], 3,padding=1)),\n",
    "            ('norm1', nn.BatchNorm1d(cos[2])),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('conv3', nn.Conv1d(cos[2],cos[2], 5,stride=1,padding=2)),\n",
    "            ]))\n",
    "        self.branch4 =nn.Sequential(OrderedDict([\n",
    "            #('pool',nn.MaxPool1d(2)),\n",
    "            ('conv3', nn.Conv1d(cin,cos[3], 3,stride=1,padding=1)),\n",
    "            ]))\n",
    "    def forward(self,x):\n",
    "        branch1=self.branch1(x)\n",
    "        branch2=self.branch2(x)\n",
    "        branch3=self.branch3(x)\n",
    "        branch4=self.branch4(x)\n",
    "        result=self.activa(t.cat((branch1,branch2,branch3,branch4),1))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__() \n",
    "        emb_outputs = []\n",
    "        \n",
    "        cols = ['creative_id', 'ad_id', 'advertiser_id', 'product_id', 'industry']\n",
    "        n_in = len(cols)\n",
    "        for i in range(n_in):\n",
    "            We = np.load('./w2v_256_120/{}_embedding_weight.npy'.format(cols[i]))\n",
    "            We = np.vstack([We, np.zeros(256)])\n",
    "            embed = nn.Embedding(num_embeddings=We.shape[0],embedding_dim=We.shape[1],padding_idx=len(We)-1, _weight=t.FloatTensor(We))\n",
    "            for p in embed.parameters(): \n",
    "                p.requires_grad=False\n",
    "            emb_outputs.append(embed)\n",
    "            \n",
    "        for i in range(n_in):\n",
    "            We = np.load('./w2v_128_60/{}_embedding_weight.npy'.format(cols[i]))\n",
    "            We = np.vstack([We, np.zeros(128)])\n",
    "            embed = nn.Embedding(num_embeddings=We.shape[0],embedding_dim=We.shape[1],padding_idx=len(We)-1, _weight=t.FloatTensor(We))\n",
    "            for p in embed.parameters(): \n",
    "                p.requires_grad=False\n",
    "            emb_outputs.append(embed)\n",
    "            del We\n",
    "            gc.collect()\n",
    "\n",
    "        self.encoders = nn.ModuleList(emb_outputs)\n",
    "        self.emb_drop = nn.Dropout(p=0.2)\n",
    "        self.embed_conv=nn.Sequential(\n",
    "            Inception(1920,1024),#(batch_size,64,opt.title_seq_len)->(batch_size,32,(opt.title_seq_len)/2)\n",
    "            Inception(1024,1024),\n",
    "            #nn.MaxPool1d(opt.title_seq_len)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(1024*2, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(1024,n_cls)\n",
    "        )\n",
    "\n",
    "    def forward(self, xs):\n",
    "        inp = [self.encoders[i](x) for i, x in enumerate(xs)] + [self.encoders[i + 5](x) for i, x in enumerate(xs)]\n",
    "        x = t.cat(inp, 2)\n",
    "        x = self.emb_drop(x)\n",
    "        x = self.embed_conv(x.permute(0,2,1))\n",
    "        x = t.max(x.permute(0,2,1), dim=1)[0]\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(optimizer, grad_clip):\n",
    "    for group in optimizer.param_groups:\n",
    "        #print(group['params'])\n",
    "        for param in group['params']:\n",
    "            param.grad.data.clamp_(-grad_clip, grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(t.utils.data.Dataset):\n",
    "    def __init__(self, xs, y, shuffle=False):\n",
    "        self.xs = xs\n",
    "        self.y = y\n",
    "        self.size = len(xs[0])\n",
    "        self.shuffle = shuffle\n",
    "         \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        xs = [x[idx].astype(np.int64) for x in self.xs]\n",
    "        y = self.y[idx]\n",
    "        if self.shuffle and np.random.rand() < 0.8:\n",
    "            state = np.random.get_state()\n",
    "            for x in xs:\n",
    "                np.random.set_state(state)\n",
    "                np.random.shuffle(x)     \n",
    "        return xs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    t0 = time.time()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.train() \n",
    "    for i, (xs, y) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        xs = [t.LongTensor(x).cuda() for x in xs]\n",
    "        yp = model(xs)\n",
    "        loss = criterion(yp, t.LongTensor(y).cuda())\n",
    "        loss.backward()\n",
    "        clip_gradient(optimizer, 0.1)\n",
    "        optimizer.step()\n",
    "        yp = t.softmax(yp, 1)\n",
    "        yp = np.argmax(yp.detach().cpu().numpy(), axis=1)\n",
    "        y_pred.append(yp)\n",
    "        y_true.append(y)\n",
    "        print('process: [%d/%d]' % (i + 1, len(loader)), end='\\r')\n",
    "    y_true = np.hstack(y_true)\n",
    "    y_pred = np.hstack(y_pred)\n",
    "    score = accuracy_score(y_true, y_pred)\n",
    "    print('process: [%d/%d], score: %f, dtime: %ds' % (i + 1, len(loader), score, time.time()-t0), end='\\n')\n",
    "    return None\n",
    "\n",
    "def val(model, loader):\n",
    "    t0 = time.time()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        for i, (xs, y) in enumerate(loader):\n",
    "            xs = [t.LongTensor(x).cuda() for x in xs]\n",
    "            yp = model(xs)\n",
    "            yp = t.softmax(yp, 1)\n",
    "            yp = np.argmax(yp.cpu().numpy(), axis=1)\n",
    "            y_pred.append(yp)\n",
    "            y_true.append(y)\n",
    "            print('process: [%d/%d]' % (i + 1, len(loader)), end='\\r')\n",
    "    model.train() \n",
    "    y_true = np.hstack(y_true)\n",
    "    y_pred = np.hstack(y_pred)\n",
    "    score = accuracy_score(y_true, y_pred)\n",
    "    print('process: [%d/%d], score: %f, dtime: %ds' % (i + 1, len(loader), score, time.time()-t0), end='\\n')\n",
    "    return y_pred, score\n",
    "\n",
    "def test(model, loader, y_test):\n",
    "    t0 = time.time()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with t.no_grad():\n",
    "        for i, (xs,y) in enumerate(loader):\n",
    "            xs = [t.LongTensor(x).cuda() for x in xs]\n",
    "            yp = model(xs)\n",
    "            yp = t.softmax(yp, 1)\n",
    "            y_test[i*batch_size:(i+1)*batch_size] = yp.cpu().numpy()\n",
    "            print('process: [%d/%d]' % (i + 1, len(loader)), end='\\r')\n",
    "    model.train() \n",
    "    print('process: [%d/%d], dtime: %ds' % (i + 1, len(loader), time.time()-t0), end='\\n')\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-07-19 03:50:02,444][<ipython-input-10-6dec02038b62>][line:12][INFO] fold 0 start training!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process: [2344/2344], score: 0.438579, dtime: 1300s\n",
      "process: [586/586]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-07-19 04:14:05,076][<ipython-input-10-6dec02038b62>][line:29][INFO] Epoch:[0/20]\t score=0.47281\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process: [586/586], score: 0.472813, dtime: 115s\n",
      "process: [977/977], dtime: 190s\n",
      "best score: 0.47281\n",
      "process: [2344/2344], score: 0.470395, dtime: 1296s\n",
      "process: [586/586]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-07-19 04:40:47,735][<ipython-input-10-6dec02038b62>][line:29][INFO] Epoch:[1/20]\t score=0.47964\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process: [586/586], score: 0.479637, dtime: 115s\n",
      "process: [977/977], dtime: 190s\n",
      "best score: 0.47964\n",
      "process: [2344/2344], score: 0.480679, dtime: 1295s\n",
      "process: [586/586]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-07-19 05:07:28,655][<ipython-input-10-6dec02038b62>][line:29][INFO] Epoch:[2/20]\t score=0.48953\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process: [586/586], score: 0.489527, dtime: 114s\n",
      "process: [324/977]\r"
     ]
    }
   ],
   "source": [
    "tmp_data = pd.read_csv('./data/train_preliminary/user.csv')\n",
    "y = label['age'].values-1\n",
    "n_cls = 10\n",
    "batch_size = 1024\n",
    "y_test = np.zeros([mats_test[0].shape[0], n_cls])\n",
    "loader_te = DataLoader(Dataset(mats_test, np.zeros(mats_test[0].shape[0])), batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1995)\n",
    "best_scores = []\n",
    "logger = get_logger('./torch2.log')\n",
    "\n",
    "for i, (idx_trn, idx_val) in enumerate(kfold.split(np.zeros((mats_train[0].shape[0], 1)))):\n",
    "    logger.info('fold {} start training!'.format(i))\n",
    "    x_test = np.zeros([mats_test[0].shape[0], n_cls])\n",
    "    x_trn = [mat[idx_trn] for mat in mats_train]\n",
    "    x_val = [mat[idx_val] for mat in mats_train]\n",
    "    y_trn, y_val = y[idx_trn], y[idx_val]\n",
    "    model = CNN().cuda()\n",
    "    optimizer = t.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = 0.001)\n",
    "    scheduler = t.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loader_trn = DataLoader(Dataset(x_trn, y_trn, shuffle=True), batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    loader_val = DataLoader(Dataset(x_val, y_val), batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    best_score = 0.0\n",
    "    for i in range(20):\n",
    "        if i > 10:\n",
    "            scheduler.step()\n",
    "        train(model, loader_trn, optimizer, criterion)\n",
    "        result, score = val(model, loader_val)\n",
    "        logger.info('Epoch:[{}/{}]\\t score={:.5f}\\t'.format(i , 20, score))\n",
    "        if score > best_score:\n",
    "            x_test = test(model, loader_te, x_test)\n",
    "            best_score = score\n",
    "            print('best score: %.5f' % score)\n",
    "    best_scores.append(best_score)\n",
    "    y_test += x_test\n",
    "print('best scores:', best_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./age_res/torch_cnn2_test_pred.npy', y_test/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.508359"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(best_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999922224582"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test[0]/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
